{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import io\n",
    "import ast\n",
    "import sys\n",
    "import json\n",
    "import boto3 \n",
    "import pymongo \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from PIL import Image\n",
    "from urllib import request\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "import cabutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projName = \"mlve\"\n",
    "experimentName = \"gestalt_static_localization\"\n",
    "S3_BUCKET_NAME = \"gestalt-scenes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading data from ec2 server (mostly just instructions for thomas)\n",
    "\n",
    "In `settings.conf` change the `MONGODB_PORT` to 8000, and the `MONGODB_HOST` to `localhost`. Then run the ssh port into the ec2 server: \n",
    "\n",
    "```\n",
    "ssh -i path/to/pem/key/maybe-named-something-like/Cocosci_WebExperiments.pem -fNL 8000:localhost:27017 ubuntu@ec2-54-91-252-25.compute-1.amazonaws.com\n",
    "```\n",
    "\n",
    "Change the path to the pem key, but otherwise this should all stay the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = cabutils.get_db_connection()\n",
    "db = conn[projName + \"_outputs\"]\n",
    "col = db[experimentName]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_to_df():\n",
    "    results = []\n",
    "    cursor = col.find({})\n",
    "    for document in cursor:\n",
    "        results.append(document)\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "df = results_to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_from_s3(url, resource_type=\"image\"):\n",
    "    s3 = boto3.resource('s3', region_name=\"us-east-2\")\n",
    "    bucket = s3.Bucket(S3_BUCKET_NAME)\n",
    "    item = bucket.Object(url)\n",
    "    if resource_type == \"image\":\n",
    "        file_stream = io.BytesIO()\n",
    "        item.download_fileobj(file_stream)\n",
    "        img = Image.open(file_stream)\n",
    "        return img\n",
    "    \n",
    "    else:\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_from_url(url):\n",
    "    obj = request.urlretrieve(url)\n",
    "    image = Image.open(obj[0])\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_box(box):\n",
    "    \"\"\"\n",
    "    Expects box as a tuple, of start stop points. Returns it as: [minX, minY, maxX, maxY]\n",
    "    \"\"\"\n",
    "    minX = min(box[0][0], box[1][0])\n",
    "    maxX = max(box[0][0], box[1][0])\n",
    "    \n",
    "    minY = min(box[0][1], box[1][1])\n",
    "    maxY = max(box[0][1], box[1][1])\n",
    "    \n",
    "    return [int(minX), int(minY), int(maxX), int(maxY)]\n",
    "\n",
    "def calc_mIOU(box_a, box_b, epsilon=1e-8):\n",
    "    if not box_a and not box_b:\n",
    "        return float(\"nan\")\n",
    "    if not box_a or not box_a:\n",
    "        return 0\n",
    "    \n",
    "    if len(box_a) < 4:\n",
    "        a = process_box(box_a)\n",
    "    else:\n",
    "        a = box_a\n",
    "    \n",
    "    if len(box_b) < 4:\n",
    "        b = process_box(box_b)\n",
    "    else:\n",
    "        b = box_b\n",
    "        \n",
    "    # return np.sqrt((a[0] - b[0]) ** 2 + (a[1] - b[1]) ** 2)\n",
    "\n",
    "    # COORDINATES OF THE INTERSECTION BOX\n",
    "    \n",
    "    xA = max(a[0], b[0])\n",
    "    yA = max(a[1], b[1])\n",
    "    xB = min(a[2], b[2])\n",
    "    yB = min(a[3], b[3])\n",
    "    \n",
    "    # The intersection of two axis-aligned bounding boxes is always an\n",
    "    # axis-aligned bounding box\n",
    "    intersection_area = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "\n",
    "    # compute the area of both AABBs\n",
    "    bb1_area = (a[2] - a[0] + 1) * (a[3] - a[1] + 1)\n",
    "    bb2_area = (b[2] - b[0] + 1) * (b[3] - b[1] + 1)\n",
    "    if intersection_area == (bb1_area + bb2_area):\n",
    "        return float('nan')\n",
    "    \n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\n",
    "    assert iou >= 0.0\n",
    "    assert iou <= 1.0\n",
    "    return iou\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(EXPERIMENT_NAME + \"-results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"correct\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate cleaned version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's quickly look at the comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comments = df[df[\"trial_type\"] == \"survey-text\"][\"response\"]\n",
    "comments = []\n",
    "for response in all_comments:\n",
    "    comm = response[\"Q0\"]\n",
    "    if len(comm) > 0:\n",
    "        comments.append(comm)\n",
    "print(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df[\"trial_type\"] != \"object-localization-task\"].index, inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants = df.groupby(\"userID\")\n",
    "participants_failed = []\n",
    "i = 0 \n",
    "\n",
    "for index, user_results in participants:\n",
    "    i += 1\n",
    "    if len(user_results) < 100:\n",
    "        print(\"USER ID: \" + user_results[\"userID\"].iloc[0] + \" did not finish the experiment\")\n",
    "        participants_failed.append(user_results[\"userID\"].iloc[0])\n",
    "        continue\n",
    "    \n",
    "    if user_results[\"correct\"].mean() < 0.5:\n",
    "        print(\"USER ID: \" + user_results[\"userID\"].iloc[0] + \" did worse than 50%\")\n",
    "        participants_failed.append(user_results[\"userID\"].iloc[0])\n",
    "        continue\n",
    "        \n",
    "    attention_checks = user_results[user_results[\"stimulus\"].str.contains(\"ground_truth\")]\n",
    "    if attention_checks[\"correct\"].sum() < 2:\n",
    "        print(\"USER ID: \" + user_results[\"userID\"].iloc[0] + \" faled too many attention checks\")\n",
    "        participants_failed.append(user_results[\"userID\"].iloc[0])\n",
    "\n",
    "# participants_failed.append(\"60cb62a4e099dcbffe591021\")\n",
    "# participants_failed.append(\"5ec3d06016290204b072564d\")\n",
    "print(participants_failed)\n",
    "\n",
    "failed_participants = df[\"userID\"].apply(lambda x: x in participants_failed)\n",
    "df = df[~failed_participants]\n",
    "attention_checks = df[\"stimulus\"].apply(lambda x: \"ground_truth\" in x)\n",
    "df = df[~attention_checks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df[\"texture_name\"] == \"\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create directory for figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"figures/{experimentName}\"):\n",
    "    os.makedirs(f\"figures/{experimentName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create texture_name column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"texture_name\"] = df[\"stimulus\"].apply(lambda x: x.split(\".com/\")[1].split(\"/\")[0].split(\"_\")[1])\n",
    "df[\"texture_name\"] = df[\"texture_name\"].apply(lambda x: \"Dots\" if x == \"voronoi\" else x.capitalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ious = []\n",
    "for index, row in df.iterrows():\n",
    "    gtbb = row[\"gt_bounding_box\"]\n",
    "    if type(gtbb) == str:\n",
    "        gtbb = str_to_list(gtbb)\n",
    "    ubb = row[\"subject_bounding_box\"]\n",
    "    if type(ubb) == str:\n",
    "        ubb = str_to_list(ubb)\n",
    "\n",
    "    if (not ubb and not gtbb) or (ubb and not gtbb):\n",
    "        ious.append(float(\"nan\"))\n",
    "        continue \n",
    "\n",
    "    elif (not ubb and gtbb):\n",
    "        ious.append(0)\n",
    "        continue\n",
    "\n",
    "    else:\n",
    "        iou = calc_mIOU(gtbb, ubb)\n",
    "        ious.append(iou)\n",
    "\n",
    "df[\"miou\"] = ious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_means = df.groupby(\"userID\")[\"miou\"].mean()\n",
    "low_miou = user_means[user_means < 0.3]\n",
    "low_miou_user_ids = [x for x in low_miou.keys()]\n",
    "all_participants_failed = participants_failed + low_miou_user_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants_failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_miou_user_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_participants_failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.barplot(y=\"miou\", x=\"userID\", data=df)\n",
    "g.set(xticklabels=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"miou\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "textures = []\n",
    "target = \"userID\"\n",
    "challenging_stimuli = []\n",
    "for index, trials in df.groupby(target): \n",
    "    accuracy = trials[\"correct\"].mean()\n",
    "    \n",
    "    if accuracy < 0.5:\n",
    "        challenging_stimuli.append((accuracy, trials[\"stimulus\"].iloc[0]))\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    \n",
    "figure = plt.figure(figsize=(12,8))\n",
    "sns.histplot(x=accuracies, binwidth=.05)\n",
    "plt.xlim(0, 1)\n",
    "plt.xlabel(\"Mean Accuracy\")\n",
    "plt.ylabel(f\"Count (n={len(accuracies)})\")\n",
    "plt.title(f\"Accuracy per-stimulus across all users\")\n",
    "# plt.savefig(f\"figures/{experimentName}/mean_accuracy_hist_per_{target}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "textures = []\n",
    "target = \"stimulus\"\n",
    "challenging_stimuli = []\n",
    "for index, trials in df.groupby(target): \n",
    "    accuracy = trials[\"correct\"].mean()\n",
    "    \n",
    "    if accuracy < 0.5:\n",
    "        challenging_stimuli.append((accuracy, trials[\"stimulus\"].iloc[0]))\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    \n",
    "figure = plt.figure(figsize=(12,8))\n",
    "sns.histplot(x=accuracies, binwidth=.05)\n",
    "plt.xlim(0, 1)\n",
    "plt.xlabel(\"Mean Accuracy\")\n",
    "plt.ylabel(f\"Count (n={len(accuracies)})\")\n",
    "plt.title(f\"Accuracy per-user across stimulus\")\n",
    "# plt.savefig(f\"figures/{experimentName}/mean_accuracy_hist_per_{target}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "challenging_stimuli = []\n",
    "for index, trials in df.groupby([\"userID\", \"n_objs\"]): \n",
    "    \n",
    "    accuracy = trials[\"correct\"].mean()\n",
    "    user_id = trials[\"userID\"].iloc[0]\n",
    "    texture_name = trials[\"n_objs\"].iloc[0]\n",
    "    \n",
    "    data.append({\"accuracy\": accuracy, \"userID\": user_id, \"texture\": texture_name})\n",
    "    \n",
    "    if accuracy < 0.5:\n",
    "        challenging_stimuli.append((accuracy, trials[\"stimulus\"].iloc[0]))\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "\n",
    "tempdf = pd.DataFrame(data)\n",
    "\n",
    "figure, ax = plt.subplots(figsize=(12,8))\n",
    "ax = sns.boxplot(y=\"accuracy\", x=\"texture\", data=tempdf)\n",
    "ax = sns.swarmplot(y=\"accuracy\", x=\"texture\", color=\"black\", alpha=0.5, data=tempdf)\n",
    "\n",
    "plt.ylabel(\"Mean Accuracy\")\n",
    "plt.xlabel(f\"Number of Objects in scene\")\n",
    "plt.title(f\"Accuracy by # objects across all users (n={len(df['userID'].unique())})\")\n",
    "# plt.savefig(f\"figures/{EXPERIMENT_NAME}/mean_accuracy_hist_per_{target}.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "challenging_stimuli = []\n",
    "for index, trials in df.groupby([\"stimulus\", \"n_objs\"]): \n",
    "    \n",
    "    accuracy = trials[\"correct\"].mean()\n",
    "    stimulus_id = trials[\"stimulus\"].iloc[0]\n",
    "    n_objs = trials[\"n_objs\"].iloc[0]\n",
    "    \n",
    "    data.append({\"accuracy\": accuracy, \"stimulus\": stimulus_id, \"n_objs\": n_objs})\n",
    "    \n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "\n",
    "tempdf = pd.DataFrame(data)\n",
    "\n",
    "figure, ax = plt.subplots(figsize=(12,8))\n",
    "ax = sns.boxplot(y=\"accuracy\", x=\"n_objs\", data=tempdf)\n",
    "ax = sns.stripplot(y=\"accuracy\", x=\"n_objs\", color=\"black\", alpha=0.5, data=tempdf)\n",
    "\n",
    "plt.ylabel(\"Mean Accuracy\")\n",
    "plt.xlabel(f\"Number of Objects in scene\")\n",
    "plt.title(f\"Accuracy by # objects across stimulus (n={len(df['stimulus'].unique())})\")\n",
    "# plt.savefig(f\"figures/{EXPERIMENT_NAME}/mean_accuracy_hist_per_{target}.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy across all textures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "textures = []\n",
    "target = \"stimulus\"\n",
    "\n",
    "for index, trials in df.groupby(target): \n",
    "    accuracy = trials[\"correct\"].mean()\n",
    "    accuracies.append(accuracy)\n",
    "    if trials[\"texture_name\"].iloc[0] == \"\":\n",
    "        continue\n",
    "    textures.append(trials[\"texture_name\"].iloc[0])\n",
    "    \n",
    "    \n",
    "tempdf = pd.DataFrame({\"textures\": textures, \"accuracies\": accuracies})\n",
    "\n",
    "figure = plt.figure(figsize=(12,8))\n",
    "sns.boxplot(y=\"accuracies\", x=\"textures\", data=tempdf, order=[\"Dots\", \"Noise\", \"Wave\"])\n",
    "\n",
    "plt.xlabel(\"Texture type (40 stimuli per texture)\")\n",
    "plt.ylabel(f\"Accuracy per stimuli\")\n",
    "plt.title(f\"Accuracy by texture type across stimulus\")\n",
    "# plt.savefig(f\"figures/{EXPERIMENT_NAME}/mean_accuracy_hist_per_{target}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "textures = []\n",
    "target = \"userID\"\n",
    "challenging_stimuli = []\n",
    "for index, trials in df.groupby([target, \"texture_name\"]): \n",
    "    accuracy = trials[\"correct\"].mean()\n",
    "\n",
    "    \n",
    "    accuracies.append(accuracy)\n",
    "    textures.append(trials[\"texture_name\"].iloc[0])\n",
    "    \n",
    "    \n",
    "tempdf = pd.DataFrame({\"textures\": textures, \"accuracies\": accuracies})\n",
    "\n",
    "figure = plt.figure(figsize=(12,8))\n",
    "sns.boxplot(y=\"accuracies\", x=\"textures\", data=tempdf, order=[\"Dots\", \"Noise\", \"Wave\"])\n",
    "\n",
    "plt.xlabel(\"Texture type (40 stimuli per texture)\")\n",
    "plt.ylabel(f\"Accuracy per stimuli\")\n",
    "plt.title(f\"Accuracy by texture type across users\")\n",
    "# plt.savefig(f\"figures/{EXPERIMENT_NAME}/mean_accuracy_hist_per_{target}.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  mIOU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "textures = []\n",
    "target = \"userID\"\n",
    "challenging_stimuli = []\n",
    "for index, trials in df.groupby(target): \n",
    "    accuracy = trials[\"miou\"].mean()\n",
    "    \n",
    "    if accuracy < 0.5:\n",
    "        challenging_stimuli.append((accuracy, trials[\"stimulus\"].iloc[0]))\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    \n",
    "figure = plt.figure(figsize=(12,8))\n",
    "sns.histplot(x=accuracies, binwidth=.05)\n",
    "plt.xlim(0, 1)\n",
    "plt.xlabel(\"mIOU\")\n",
    "plt.ylabel(f\"Count (n={len(accuracies)})\")\n",
    "plt.title(f\"mIOU per-stimulus across all users\")\n",
    "plt.savefig(f\"figures/{experimentName}/mean_miou_hist_per_{target}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "textures = []\n",
    "target = \"stimulus\"\n",
    "challenging_stimuli = []\n",
    "for index, trials in df.groupby(target): \n",
    "    accuracy = trials[\"miou\"].mean()\n",
    "    \n",
    "    if accuracy < 0.25:\n",
    "        challenging_stimuli.append((accuracy, trials[\"stimulus\"].iloc[0]))\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    \n",
    "figure = plt.figure(figsize=(12,8))\n",
    "sns.histplot(x=accuracies, binwidth=.05)\n",
    "plt.xlim(0, 1)\n",
    "plt.xlabel(\"mIOU\")\n",
    "plt.ylabel(f\"Count (n={len(accuracies)})\")\n",
    "plt.title(f\"mIOU per-user across all stimulus\")\n",
    "plt.savefig(f\"figures/{experimentName}/mean_miou_hist_per_{target}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenging_stimuli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "challenging_stimuli = []\n",
    "for index, trials in df.groupby([\"userID\", \"n_objs\"]): \n",
    "    \n",
    "    accuracy = trials[\"miou\"].mean()\n",
    "    user_id = trials[\"userID\"].iloc[0]\n",
    "    texture_name = trials[\"n_objs\"].iloc[0]\n",
    "    \n",
    "    data.append({\"accuracy\": accuracy, \"user_id\": user_id, \"texture\": texture_name})\n",
    "    \n",
    "    if accuracy < 0.5:\n",
    "        challenging_stimuli.append((accuracy, trials[\"stimulus\"].iloc[0]))\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "\n",
    "tempdf = pd.DataFrame(data)\n",
    "\n",
    "figure, ax = plt.subplots(figsize=(12,8))\n",
    "ax = sns.boxplot(y=\"accuracy\", x=\"texture\", data=tempdf)\n",
    "ax = sns.swarmplot(y=\"accuracy\", x=\"texture\", color=\"black\", alpha=0.5, data=tempdf)\n",
    "\n",
    "plt.ylabel(\"mIOU\")\n",
    "plt.xlabel(f\"Number of Objects in scene\")\n",
    "plt.title(f\"mIOU by # objects across all users (n={len(df['userID'].unique())})\")\n",
    "plt.savefig(f\"figures/{experimentName}/mean_accuracy_hist_per_{target}.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "challenging_stimuli = []\n",
    "for index, trials in df.groupby([\"stimulus\", \"n_objs\"]): \n",
    "    \n",
    "    accuracy = trials[\"miou\"].mean()\n",
    "    user_id = trials[\"userID\"].iloc[0]\n",
    "    texture_name = trials[\"n_objs\"].iloc[0]\n",
    "    \n",
    "    data.append({\"accuracy\": accuracy, \"user_id\": user_id, \"texture\": texture_name})\n",
    "    \n",
    "    if accuracy < 0.5:\n",
    "        challenging_stimuli.append((accuracy, trials[\"stimulus\"].iloc[0]))\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "\n",
    "tempdf = pd.DataFrame(data)\n",
    "\n",
    "figure, ax = plt.subplots(figsize=(12,8))\n",
    "ax = sns.boxplot(y=\"accuracy\", x=\"texture\", data=tempdf)\n",
    "ax = sns.swarmplot(y=\"accuracy\", x=\"texture\", color=\"black\", alpha=0.5, data=tempdf)\n",
    "\n",
    "plt.ylabel(\"mIOU\")\n",
    "plt.xlabel(f\"Number of Objects in scene\")\n",
    "n_stimulus = len(df[\"stimulus\"].unique())\n",
    "plt.title(f\"mIOU by # objects across all stimulus (n={n_stimulus})\")\n",
    "plt.savefig(f\"figures/{experimentName}/mean_accuracy_hist_per_{target}.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "challenging_stimuli = []\n",
    "for index, trials in df.groupby([\"userID\", \"texture_name\"]): \n",
    "    \n",
    "    accuracy = trials[\"miou\"].mean()\n",
    "    user_id = trials[\"userID\"].iloc[0]\n",
    "    texture_name = trials[\"texture_name\"].iloc[0]\n",
    "    \n",
    "    data.append({\"accuracy\": accuracy, \"user_id\": user_id, \"texture\": texture_name})\n",
    "    \n",
    "    if accuracy < 0.5:\n",
    "        challenging_stimuli.append((accuracy, trials[\"stimulus\"].iloc[0]))\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "\n",
    "tempdf = pd.DataFrame(data)\n",
    "\n",
    "figure, ax = plt.subplots(figsize=(12,8))\n",
    "ax = sns.boxplot(y=\"accuracy\", x=\"texture\", data=tempdf)\n",
    "ax = sns.swarmplot(y=\"accuracy\", x=\"texture\", color=\"black\", alpha=0.5, data=tempdf, order=[\"Dots\", \"Noise\", \"Wave\"])\n",
    "\n",
    "plt.ylabel(\"mIOU\")\n",
    "plt.xlabel(f\"Number of Objects in scene\")\n",
    "plt.title(f\"mIOU by texture type across all users (n={len(df['userID'].unique())})\")\n",
    "plt.savefig(f\"figures/{experimentName}/mean_accuracy_hist_per_{target}.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "challenging_stimuli = []\n",
    "for index, trials in df.groupby([\"stimulus\", \"texture_name\"]): \n",
    "    \n",
    "    accuracy = trials[\"miou\"].mean()\n",
    "    user_id = trials[\"userID\"].iloc[0]\n",
    "    texture_name = trials[\"texture_name\"].iloc[0]\n",
    "    \n",
    "    data.append({\"accuracy\": accuracy, \"user_id\": user_id, \"texture\": texture_name})\n",
    "    \n",
    "    if accuracy < 0.5:\n",
    "        challenging_stimuli.append((accuracy, trials[\"stimulus\"].iloc[0]))\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "\n",
    "tempdf = pd.DataFrame(data)\n",
    "\n",
    "figure, ax = plt.subplots(figsize=(12,8))\n",
    "ax = sns.boxplot(y=\"accuracy\", x=\"texture\", data=tempdf, order=[\"Dots\", \"Noise\", \"Wave\"])\n",
    "ax = sns.swarmplot(y=\"accuracy\", x=\"texture\", color=\"black\", alpha=0.5, data=tempdf, order=[\"Dots\", \"Noise\", \"Wave\"])\n",
    "\n",
    "plt.ylabel(\"mIOU\")\n",
    "plt.xlabel(f\"Number of Objects in scene\")\n",
    "plt.title(f\"mIOU by texture type across all stimulus (n={n_stimulus})\")\n",
    "plt.savefig(f\"figures/{experimentName}/mean_accuracy_hist_per_{target}.png\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_box(box, ax, alpha=0.8, color=\"red\", linewidth=5):\n",
    "    if len(box) != 4:\n",
    "        box = process_box(box)\n",
    "    width = box[2] - box[0]\n",
    "    height = box[3] - box[1]\n",
    "    if not color or color == \"random\":\n",
    "        import random\n",
    "        color = \"#%06x\" % random.randint(0, 0xFFFFFF)\n",
    "        \n",
    "    rect = patches.Rectangle((box[0], box[1]), width, height, linewidth=linewidth, edgecolor=color, \n",
    "                             alpha=alpha, facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    return ax\n",
    "\n",
    "def plot_trial(trial):\n",
    "    fig, ax = plt.subplots(figsize=(16,12))\n",
    "    print(trial[\"userID\"])\n",
    "    \n",
    "    gt_bb = trial[\"gt_bounding_box\"]\n",
    "    s_bb = trial[\"subject_bounding_box\"]\n",
    "    \n",
    "    print(gt_bb, s_bb)\n",
    "    if gt_bb:\n",
    "        add_box(gt_bb, ax, color=\"blue\", alpha=0.75)\n",
    "        \n",
    "    if s_bb:\n",
    "        add_box(s_bb, ax, color=\"red\", alpha=0.75)\n",
    "        \n",
    "    url = trial[\"stimulus\"]\n",
    "    image = download_from_url(url)\n",
    "\n",
    "    probe_location = trial[\"probe_location\"]\n",
    "    \n",
    "    ax.imshow(image, cmap=\"gray\")\n",
    "\n",
    "    # Add probes\n",
    "    probe = patches.Circle(probe_location, radius=12, color=\"#69597f\", alpha=0.8)\n",
    "    ax.add_patch(probe)\n",
    "    probe = patches.Circle(probe_location, radius=4, color=\"#ff0601\", alpha=1)\n",
    "    ax.add_patch(probe)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for index, trial in df.iterrows():\n",
    "    miou = calc_mIOU(trial[\"gt_bounding_box\"], trial[\"subject_bounding_box\"])\n",
    "    print(miou)\n",
    "    fig=plot_trial(trial)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "avg_bboxes = False\n",
    "run_plots = True\n",
    "\n",
    "save_path = \"/Users/yoni/Desktop/static_detection_v1_user_bboxes/\"\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "    \n",
    "if run_plots:\n",
    "    for index, trials in df.groupby(\"stimulus\"):\n",
    "\n",
    "        url = index\n",
    "        image = download_from_url(url)\n",
    "\n",
    "        # Add probes\n",
    "        probe_location = trials.iloc[0][\"probe_location\"]\n",
    "        probe_touching = trials.iloc[0][\"probe_touching\"]\n",
    "\n",
    "        if not probe_touching:\n",
    "            continue \n",
    "\n",
    "        fig, ax = plt.subplots(1,1, figsize=(16, 12))\n",
    "        image = download_from_url(url)\n",
    "\n",
    "        ax.imshow(image, cmap=\"gray\")\n",
    "\n",
    "        probe = patches.Circle(probe_location, radius=12, color=\"#69597f\", alpha=0.8)\n",
    "        ax.add_patch(probe)\n",
    "\n",
    "        probe = patches.Circle(probe_location, radius=4, color=\"#ff0601\", alpha=1)\n",
    "        ax.add_patch(probe)\n",
    "\n",
    "        gt_bb = trials.iloc[0][\"gt_bounding_box\"]\n",
    "        ax = add_box(gt_bb, ax, color=\"red\", linewidth=10, alpha=1)\n",
    "\n",
    "        avg_bb = []\n",
    "        count = 0\n",
    "        for i, trial in trials.iterrows():\n",
    "            s_bb = trial[\"subject_bounding_box\"]\n",
    "            if s_bb:\n",
    "                if avg_bboxes:\n",
    "                    box = np.array(process_box(s_bb))\n",
    "                    count += 1\n",
    "                    if not len(avg_bb):\n",
    "                        avg_bb.append(box)\n",
    "                    else:\n",
    "                        avg_bb.append(box)\n",
    "                else: \n",
    "                    add_box(s_bb, ax, alpha=0.5, linewidth=5, color=\"random\")\n",
    "\n",
    "        if avg_bboxes:\n",
    "            avg_bb = np.array(avg_bb).mean(axis=0)\n",
    "            add_box(avg_bb, ax, alpha=0.9, linewidth=5, color=\"blue\")\n",
    "            miou = calc_mIOU(avg_bb, gt_bb)\n",
    "            print(miou)\n",
    "        \n",
    "\n",
    "        trial_name = url.split(\".com\")[1][1:].replace(\"/\", \"-\")\n",
    "        trial_name = trial_name.replace(\"-images\", \"\")\n",
    "\n",
    "        plt.title(trial_name)\n",
    "        plt.savefig(os.path.join(save_path, trial_name + \".png\"))\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regress parameters that make this difficult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm_api\n",
    "import statsmodels.api as sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_filtered.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_loc = np.array([18.5, -18.2, 12.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract texture params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object parameters\n",
    "get_distortion = lambda x: x[\"params\"].get(\"Distortion\", x[\"params\"].get(\"Scale\"))\n",
    "\n",
    "attention_filtered[\"obj_scale\"] = attention_filtered[\"obj_texture_data\"].apply(lambda x: x[\"params\"][\"Scale\"])\n",
    "attention_filtered[\"obj_distortion\"] = attention_filtered[\"obj_texture_data\"].apply(get_distortion)\n",
    "\n",
    "# Background parameters\n",
    "attention_filtered[\"background_scale\"] = attention_filtered[\"background_texture\"].apply(\n",
    "    lambda x: x[\"params\"][\"Scale\"])\n",
    "attention_filtered[\"background_distortion\"] = attention_filtered[\"background_texture\"].apply(get_distortion)\n",
    "\n",
    "camera_dist = lambda x: np.sqrt(np.sum((np.array(x) - camera_loc) ** 2))\n",
    "attention_filtered[\"obj_distance\"] = attention_filtered[\"obj_location_data\"].apply(camera_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_filtered[\"scale_diff\"] = attention_filtered[\"obj_scale\"] - attention_filtered[\"background_scale\"]\n",
    "attention_filtered[\"distortion_diff\"] = attention_filtered[\"obj_distortion\"] - \\\n",
    "                                        attention_filtered[\"background_distortion\"]\n",
    "attention_filtered[\"texture_categorical\"] = attention_filtered[\"texture_name\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_filtered[\"scale_diff\"] = (attention_filtered[\"scale_diff\"] - attention_filtered[\"scale_diff\"].mean()) / \\\n",
    "                                    attention_filtered[\"scale_diff\"].std()\n",
    "    \n",
    "attention_filtered[\"distortion_diff\"] = (attention_filtered[\"distortion_diff\"] - attention_filtered[\"distortion_diff\"].mean()) / \\\n",
    "                                    attention_filtered[\"distortion_diff\"].std()\n",
    "    \n",
    "attention_filtered[\"miou_norm\"] = (attention_filtered[\"miou\"] - attention_filtered[\"miou\"].mean()) / \\\n",
    "                                    attention_filtered[\"miou\"].std()\n",
    "    \n",
    "attention_filtered[\"obj_distance\"] = (attention_filtered[\"obj_distance\"] - attention_filtered[\"obj_distance\"].mean()) / \\\n",
    "                                    attention_filtered[\"obj_distance\"].std()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "exog_vars = [\"obj_distance\", \"scale_diff\", \"distortion_diff\"]\n",
    "exog = attention_filtered[exog_vars]\n",
    "sns.heatmap(exog.corr(), cmap=\"viridis\")\n",
    "plt.show()\n",
    "\n",
    "keep_idxs = attention_filtered[\"miou\"].notna()\n",
    "X = sm.add_constant(exog[keep_idxs])\n",
    "endog = attention_filtered[\"miou_norm\"][keep_idxs]\n",
    "model = sm.OLS(exog=X, endog=endog)\n",
    "result = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(10, 8))\n",
    "fig = sm.graphics.plot_partregress_grid(result)\n",
    "fig.tight_layout(pad=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
