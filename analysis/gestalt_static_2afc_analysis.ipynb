{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import io\n",
    "import ast\n",
    "import sys\n",
    "import json\n",
    "import boto3 \n",
    "import pymongo \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from PIL import Image\n",
    "from urllib import request\n",
    "from collections import Counter\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "import cabutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rcParams\n",
    "rcParams[\"font.size\"] = 18\n",
    "rcParams[\"figure.titlesize\"] = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "projName = \"thomas_m2s\"\n",
    "experimentName = \"shapenet13.v0\"\n",
    "iterName = \"v0\"\n",
    "S3_BUCKET_NAME = \"thomas-m2s-shapenet13-v0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading data from ec2 server (mostly just instructions for thomas)\n",
    "\n",
    "In `settings.conf` change the `MONGODB_PORT` to 8000, and the `MONGODB_HOST` to `localhost`. Then run the ssh port into the ec2 server: \n",
    "\n",
    "```\n",
    "ssh -i path/to/pem/key/maybe-named-something-like/Cocosci_WebExperiments.pem -fNL 8000:localhost:27017 ubuntu@ec2-54-91-252-25.compute-1.amazonaws.com\n",
    "```\n",
    "\n",
    "Change the path to the pem key, but otherwise this should all stay the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking database connection...\n",
      "Connection established!\n"
     ]
    }
   ],
   "source": [
    "conn = cabutils.get_db_connection()\n",
    "db = conn[projName + \"_outputs\"]\n",
    "col = db[experimentName]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_to_df():\n",
    "    results = []\n",
    "    cursor = col.find({})\n",
    "    for document in cursor:\n",
    "        results.append(document)\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "df = results_to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 314 entries, 0 to 313\n",
      "Data columns (total 23 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   _id               314 non-null    object \n",
      " 1   img_sample        312 non-null    object \n",
      " 2   img_target        312 non-null    object \n",
      " 3   img_lure          312 non-null    object \n",
      " 4   rt                314 non-null    int64  \n",
      " 5   index             312 non-null    float64\n",
      " 6   stimulus          312 non-null    object \n",
      " 7   response          314 non-null    object \n",
      " 8   correct           312 non-null    object \n",
      " 9   correct_choice    312 non-null    float64\n",
      " 10  choices           312 non-null    object \n",
      " 11  trial_type        314 non-null    object \n",
      " 12  trial_index       314 non-null    int64  \n",
      " 13  time_elapsed      314 non-null    int64  \n",
      " 14  internal_node_id  314 non-null    object \n",
      " 15  gameid            312 non-null    object \n",
      " 16  iterationName     314 non-null    object \n",
      " 17  inputid           0 non-null      object \n",
      " 18  projName          314 non-null    object \n",
      " 19  expName           314 non-null    object \n",
      " 20  sessionID         0 non-null      object \n",
      " 21  studyID           0 non-null      object \n",
      " 22  userID            314 non-null    object \n",
      "dtypes: float64(2), int64(3), object(18)\n",
      "memory usage: 56.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_from_s3(url, resource_type=\"image\"):\n",
    "    s3 = boto3.resource('s3', region_name=\"us-east-2\")\n",
    "    bucket = s3.Bucket(S3_BUCKET_NAME)\n",
    "    item = bucket.Object(url)\n",
    "    if resource_type == \"image\":\n",
    "        file_stream = io.BytesIO()\n",
    "        item.download_fileobj(file_stream)\n",
    "        img = Image.open(file_stream)\n",
    "        return img\n",
    "    \n",
    "    else:\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_from_url(url):\n",
    "    obj = request.urlretrieve(url)\n",
    "    image = Image.open(obj[0])\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7211538461538461"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"correct\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['tom_pilot0', 'yoni_test'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"userID\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9038461538461539"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"userID\"] == \"tom_pilot0\"][\"correct\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate cleaned version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's quickly look at the comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comments = df[df[\"trial_type\"] == \"survey-text\"][\"response\"]\n",
    "comments = []\n",
    "for response in all_comments:\n",
    "    comm = response[\"Q0\"]\n",
    "    if len(comm) > 0:\n",
    "        comments.append(comm)\n",
    "print(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df[\"trial_type\"] != \"plugin-2afc-task\"].index, inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants = df.groupby(\"userID\")\n",
    "participants_failed = []\n",
    "i = 0 \n",
    "\n",
    "for index, user_results in participants:\n",
    "    i += 1\n",
    "    if len(user_results) < 100:\n",
    "        print(\"USER ID: \" + user_results[\"userID\"].iloc[0] + \" did not finish the experiment\")\n",
    "        participants_failed.append(user_results[\"userID\"].iloc[0])\n",
    "        continue\n",
    "    \n",
    "    if user_results[\"correct\"].mean() == 0.5:\n",
    "        print(\"USER ID: \" + user_results[\"userID\"].iloc[0] + \" did exactly 50%\")\n",
    "        #participants_failed.append(user_results[\"userID\"].iloc[0])\n",
    "        continue\n",
    "        \n",
    "    attention_checks = user_results[user_results[\"stimulus\"].str.contains(\"ground_truth\")]\n",
    "    if attention_checks[\"correct\"].sum() < len(attention_checks) - 3:\n",
    "        num_failed = len(attention_checks) - attention_checks[\"correct\"].sum()\n",
    "        print(\"USER ID: \" + user_results[\"userID\"].iloc[0] + \" faled \" + str(num_failed) + \" attention checks\")\n",
    "        participants_failed.append(user_results[\"userID\"].iloc[0])\n",
    "\n",
    "print(participants_failed)\n",
    "\n",
    "failed_participants = df[\"userID\"].apply(lambda x: x in participants_failed)\n",
    "df = df[~failed_participants]\n",
    "attention_checks = df[\"stimulus\"].apply(lambda x: \"ground_truth\" in x)\n",
    "df = df[~attention_checks]\n",
    "\n",
    "my_data = df[\"userID\"] == \"yoni_test2\"\n",
    "df = df[~my_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants_failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df[df[\"userID\"] == \"5f4bccaefca4707a4ca6ba7d\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create directory for figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"figures/{experimentName}\"):\n",
    "    os.makedirs(f\"figures/{experimentName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = len(df[\"userID\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create texture_name column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"texture_name\"] = df[\"stimulus\"].apply(lambda x: x.split(\".com/\")[1].split(\"/\")[0].split(\"_\")[1])\n",
    "df[\"texture_name\"] = df[\"texture_name\"].apply(lambda x: \"Dots\" if x == \"voronoi\" else x.capitalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"texture_scale\"] = df[\"stimulus\"].apply(lambda x: float(x[x.index(\"scale=\")+6:x.index(\"scale=\")+11]))\n",
    "df[\"texture_distortion\"] = df[\"stimulus\"].apply(lambda x: float(x[x.index(\"rtion=\")+6:x.index(\"rtion=\")+11]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = df.groupby(\"texture_name\")\n",
    "max_, min_ = groups[\"texture_scale\"].transform(\"max\"), groups[\"texture_scale\"].transform(\"min\")\n",
    "df[\"texture_scale\"] = (df[\"texture_scale\"] - min_) / (max_ - min_)\n",
    "df[\"texture_scale\"] = df[\"texture_scale\"].apply(lambda x: float(f\"{x:.03f}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = df.groupby(\"texture_name\")\n",
    "max_, min_ = groups[\"texture_distortion\"].transform(\"max\"), groups[\"texture_distortion\"].transform(\"min\")\n",
    "df[\"texture_distortion\"] = (df[\"texture_distortion\"] - min_) / (max_ - min_)\n",
    "df[\"texture_distortion\"] = df[\"texture_distortion\"].apply(lambda x: float(f\"{x:.03f}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = []\n",
    "for index, row in df.iterrows():\n",
    "    choices = row[\"choices\"]\n",
    "    \n",
    "    gt_params = row[\"gt_shape_params\"]\n",
    "    alt_params = row[\"alt_shape_params\"]\n",
    "    \n",
    "    distance = np.sqrt((gt_params[0] - alt_params[0])**2 + (gt_params[1] - alt_params[1]) ** 2) \n",
    "\n",
    "    distances.append(distance)\n",
    "    \n",
    "df[\"distances\"] = distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,12))\n",
    "g = sns.barplot(y=\"correct\", x=\"userID\", data=df)\n",
    "g.set(xticklabels=[])\n",
    "plt.xlabel(\"User\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(f\"Accuracy per user (n={n_users}, mean={df['correct'].mean():.02f})\", fontsize=24)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "sns.countplot(x=\"batch\", data=df)\n",
    "plt.xticks(range(10), range(10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(16,12))\n",
    "sns.barplot(x=\"texture_scale\", y=\"correct\", hue=\"texture_name\", data=df)\n",
    "plt.xlabel(\"Texture Scale\")\n",
    "plt.xticks(range(3), [\"Small\", \"Medium\", \"Large\"])\n",
    "\n",
    "plt.ylabel(\"Correct\")\n",
    "plt.title(\"Texture Scale vs. Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(16,12))\n",
    "sns.barplot(x=\"texture_distortion\", y=\"correct\", hue=\"texture_name\", data=df)\n",
    "plt.xlabel(\"Texture Distortion\")\n",
    "plt.xticks(range(3), [\"Low\", \"Medium\", \"High\"])\n",
    "\n",
    "plt.ylabel(\"Correct\")\n",
    "plt.title(\"Texture distortion vs. Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df[\"distances\"])\n",
    "plt.title(\"Distribution of euclidean distance between distractors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "textures = []\n",
    "target = \"userID\"\n",
    "challenging_stimuli = []\n",
    "for index, trials in df.groupby(target): \n",
    "    accuracy = trials[\"correct\"].mean()\n",
    "    \n",
    "    if accuracy < 0.5:\n",
    "        challenging_stimuli.append((accuracy, trials[\"stimulus\"].iloc[0]))\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    \n",
    "figure = plt.figure(figsize=(12,8))\n",
    "sns.histplot(x=accuracies, binwidth=.05)\n",
    "plt.xlim(0, 1)\n",
    "plt.xlabel(\"Mean Accuracy\")\n",
    "plt.ylabel(f\"Count (n={len(accuracies)})\")\n",
    "plt.title(f\"Participant Accuracy\")\n",
    "# plt.savefig(f\"figures/{experimentName}/mean_accuracy_hist_per_{target}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "textures = []\n",
    "target = \"stimulus\"\n",
    "challenging_stimuli = []\n",
    "for index, trials in df.groupby(target): \n",
    "    accuracy = trials[\"correct\"].mean()\n",
    "    \n",
    "    if accuracy < 0.5:\n",
    "        challenging_stimuli.append((accuracy, trials[\"stimulus\"].iloc[0]))\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "figure = plt.figure(figsize=(12,8))\n",
    "g = sns.histplot(x=accuracies, binwidth=0.1, stat=\"probability\")\n",
    "plt.xlim(0, 1)\n",
    "\n",
    "plt.xlabel(\"Mean Accuracy\")\n",
    "plt.ylabel(f\"% of Stimuli (n={len(accuracies)})\")\n",
    "plt.title(f\"Stimulus level accuracy\")\n",
    "# plt.savefig(f\"figures/{experimentName}/mean_accuracy_hist_per_{target}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy across all textures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "textures = []\n",
    "target = \"stimulus\"\n",
    "\n",
    "for index, trials in df.groupby(target): \n",
    "    accuracy = trials[\"correct\"].mean()\n",
    "    accuracies.append(accuracy)\n",
    "    if trials[\"texture_name\"].iloc[0] == \"\":\n",
    "        continue\n",
    "    textures.append(trials[\"texture_name\"].iloc[0])\n",
    "    \n",
    "    \n",
    "tempdf = pd.DataFrame({\"textures\": textures, \"accuracies\": accuracies})\n",
    "\n",
    "figure = plt.figure(figsize=(12,8))\n",
    "sns.boxplot(y=\"accuracies\", x=\"textures\", data=tempdf, order=[\"Dots\", \"Noise\", \"Wave\"])\n",
    "\n",
    "plt.xlabel(\"Texture type\")\n",
    "plt.ylabel(f\"Accuracy per stimuli\")\n",
    "plt.title(f\"Accuracy per-image across stimuli (~5 repeats per-stimuli)\")\n",
    "# plt.savefig(f\"figures/{EXPERIMENT_NAME}/mean_accuracy_hist_per_{target}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "textures = []\n",
    "target = \"userID\"\n",
    "challenging_stimuli = []\n",
    "for index, trials in df.groupby([target, \"texture_name\"]): \n",
    "    accuracy = trials[\"correct\"].mean()\n",
    "\n",
    "    \n",
    "    accuracies.append(accuracy)\n",
    "    textures.append(trials[\"texture_name\"].iloc[0])\n",
    "    \n",
    "    \n",
    "tempdf = pd.DataFrame({\"textures\": textures, \"accuracies\": accuracies})\n",
    "\n",
    "figure = plt.figure(figsize=(12,8))\n",
    "sns.boxplot(y=\"accuracies\", x=\"textures\", data=tempdf, order=[\"Dots\", \"Noise\", \"Wave\"])\n",
    "\n",
    "plt.xlabel(f\"Texture type\")\n",
    "plt.ylabel(f\"Accuracy\")\n",
    "plt.title(f\"Accuracy across users by texture\")\n",
    "# plt.savefig(f\"figures/{EXPERIMENT_NAME}/mean_accuracy_hist_per_{target}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df[\"batch\"], binwidth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "g = sns.histplot(df[\"distances\"])\n",
    "g = sns.kdeplot(df[\"distances\"], ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter()\n",
    "for i, row in df.iterrows():\n",
    "    dist = row[\"distances\"]\n",
    "    correct = row[\"correct\"]\n",
    "    c[dist] += correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(df[\"distances\"].min(), df[\"distances\"].max(), 20)\n",
    "\n",
    "df[\"dist_binned\"] = pd.cut(df[\"distances\"], np.linspace(df[\"distances\"].min(), df[\"distances\"].max(), 20), \n",
    "                           labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"distractor_shape_type\"] = df[\"alt_shape_url\"].apply(lambda x: x.split(\"_\")[0].split(\"/\")[-1])\n",
    "df[\"shape_matching\"] = df[\"distractor_shape_type\"] == df[\"obj_shape_type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = []\n",
    "means = []\n",
    "sample_sizes = []\n",
    "matching = []\n",
    "for (stim, dist, shape_matching), row in df.groupby([\"stimulus\", \"dist_binned\", \"shape_matching\"]):\n",
    "    dists.append(dist)\n",
    "    means.append(row[\"correct\"].mean())\n",
    "    sample_sizes.append(len(row))\n",
    "    matching.append(shape_matching)\n",
    "    \n",
    "    \n",
    "tempdf = pd.DataFrame({\"dists\": dists, \"means\": means, \"Same-class distractor\": matching, \n",
    "                       \"sample_sizes\": sample_sizes})\n",
    "\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "g = sns.lmplot(x=\"dists\", y=\"means\", scatter=True, col=\"Same-class distractor\", hue=\"Same-class distractor\", data=tempdf, \n",
    "           sharex=True, sharey=True, aspect=1.5, x_jitter=0.05, line_kws={\"linewidth\": 4})\n",
    "g.set_axis_labels(\"Binned Distractor Distance\", \"Per-Stimuli Accuracy\").fig.subplots_adjust(wspace=.1)\n",
    "\n",
    "\n",
    "for ax in g.axes.flat:\n",
    "    ax.set_xticks(range(20), [f\"{b:.02f}\" for b in bins], rotation=45)\n",
    "\n",
    "plt.suptitle(f\"Per-Stimuli Accuracy by Distractor Distance  (n={len(sample_sizes) + 1})\", y=1.05)\n",
    "plt.xlabel(\"Binned Distractor Distace\")\n",
    "plt.ylabel(f\"Per-Stimuli accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = []\n",
    "means = []\n",
    "sample_sizes = []\n",
    "matching = []\n",
    "for (stim, dist, shape_matching), row in df.groupby([\"userID\", \"dist_binned\", \"shape_matching\"]):\n",
    "    dists.append(dist)\n",
    "    means.append(row[\"correct\"].mean())\n",
    "    sample_sizes.append(len(row))\n",
    "    matching.append(shape_matching)\n",
    "    \n",
    "    \n",
    "tempdf = pd.DataFrame({\"dists\": dists, \"means\": means, \"Same-class distractor\": matching, \n",
    "                       \"sample_sizes\": sample_sizes})\n",
    "\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "g = sns.lmplot(x=\"dists\", y=\"means\", scatter=True, col=\"Same-class distractor\", hue=\"Same-class distractor\", data=tempdf, \n",
    "           sharex=True, sharey=True, aspect=1.5, x_jitter=0.05, line_kws={\"linewidth\": 4})\n",
    "g.set_axis_labels(\"Binned Distractor Distance\", \"Per-User Accuracy\").fig.subplots_adjust(wspace=.1)\n",
    "\n",
    "\n",
    "for ax in g.axes.flat:\n",
    "    ax.set_xticks(range(20), [f\"{b:.02f}\" for b in bins], rotation=45)\n",
    "\n",
    "plt.suptitle(f\"Per-User Accuracy by Distractor Distance (n={n_users})\", y=1.05)\n",
    "plt.xlabel(\"Binned Distractor Distace\")\n",
    "plt.ylabel(f\"Per-User Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = []\n",
    "means = []\n",
    "sample_sizes = []\n",
    "for (user, dist), row in df.groupby([\"userID\", \"dist_binned\"]):\n",
    "    dists.append(dist)\n",
    "    means.append(row[\"correct\"].mean())\n",
    "    sample_sizes.append(len(row))\n",
    " \n",
    "\n",
    "m, b, r, p, stderr = stats.linregress(distances, correct)\n",
    "print(f\"Slope: {m:.02f}, \\nIntercept: {b:.02f}, \\nR^2: {r:.02f}, \\nP-val: {p:.05f}, \\nStd Error: {stderr:.02f}\")\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "sns.regplot(x=dists, y=means, scatter=True, x_jitter=0.1)\n",
    "plt.title(f\"Per-User Accuracy by Distractor Distance (n={n_users})\")\n",
    "plt.xticks(range(20), [f\"{b:.02f}\" for b in bins], rotation=45)\n",
    "\n",
    "plt.xlabel(\"Binned Distractor Distace\")\n",
    "plt.ylabel(\"Per-user accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = []\n",
    "means = []\n",
    "sample_sizes = []\n",
    "for dist, row in df.groupby([\"dist_binned\"]):\n",
    "    dists.append(dist)\n",
    "    means.append(row[\"correct\"].mean())\n",
    "    sample_sizes.append(len(row))\n",
    " \n",
    "\n",
    "m, b, r, p, stderr = stats.linregress(distances, correct)\n",
    "print(f\"Slope: {m:.02f}, \\nIntercept: {b:.02f}, \\nR^2: {r:.02f}, \\nP-val: {p:.05f}, \\nStd Error: {stderr:.02f}\")\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "sns.regplot(x=dists, y=means, scatter=True, x_jitter=0.1, y_jitter=0.1)\n",
    "plt.title(\"Average Accuracy by Distractor Distance\")\n",
    "plt.xticks(range(20), [f\"{b:.02f}\" for b in bins], rotation=45)\n",
    "\n",
    "plt.xlabel(\"Binned Distractor Distace\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "distances = []\n",
    "correct = []\n",
    "for dist, rows in df.groupby([\"dist_binned\"]):\n",
    "    distances.append(dist)\n",
    "    correct.append(rows[\"correct\"].mean())\n",
    "\n",
    "    \n",
    "m, b, r, p, stderr = stats.linregress(distances, correct)\n",
    "\n",
    "print(f\"Slope: {m:.02f}, \\nIntercept: {b:.02f}, \\nR^2: {r:.02f}, \\nP-val: {p:.05f}, \\nStd Error: {stderr:.02f}\")\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "sns.regplot(x=distances, y=correct, order=1)\n",
    "\n",
    "plt.xticks(range(20), [f\"{b:.02f}\" for b in bins], rotation=45)\n",
    "\n",
    "plt.xlabel(\"Distractor Distance (binned)\")\n",
    "plt.ylabel(\"Average Correct Responses\")\n",
    "plt.title(\"Accuracy by Distractor Distance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_probe(ax, location):\n",
    "    \n",
    "    probe = patches.Circle(location, radius=12, color=\"#69597f\", alpha=0.8)\n",
    "    ax.add_patch(probe)\n",
    "\n",
    "    probe = patches.Circle(location, radius=4, color=\"#ff0601\", alpha=1)\n",
    "    ax.add_patch(probe)\n",
    "    \n",
    "def view_trials(df, filter_correct=False):\n",
    "    if filter_correct:\n",
    "        df = df[df[\"correct\"] != True]\n",
    "        \n",
    "    for stimulus, rows in df.groupby(\"stimulus\"):\n",
    "        plt.close()\n",
    "        \n",
    "        row = rows.iloc[0]            \n",
    "                \n",
    "        stim_img = download_from_url(stimulus)\n",
    "        probe_location = row[\"probe_location\"]\n",
    "        \n",
    "        gt_img = download_from_url(row[\"gt_shape_url\"])\n",
    "        alt_img = download_from_url(row[\"alt_shape_url\"])\n",
    "        \n",
    "        fig = plt.figure(constrained_layout=True, figsize=(12, 12))\n",
    "        axs = fig.subplot_mosaic(\"\"\"AA\n",
    "                                    BC\n",
    "                                    \"\"\")\n",
    "        axs[\"A\"].imshow(stim_img, cmap=\"gray\")\n",
    "        add_probe(axs[\"A\"], probe_location)\n",
    "        \n",
    "        axs[\"B\"].imshow(gt_img)\n",
    "        axs[\"C\"].imshow(alt_img)\n",
    "        \n",
    "        \n",
    "        axs[\"A\"].set_title(\"Stimulus\")\n",
    "        axs[\"B\"].set_title(\"Correct Answer\")\n",
    "        axs[\"C\"].set_title(f\"Distractor Image (d={row['distances']:.03f})\")\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "view_trials(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(12,8))\n",
    "df[\"rt_seconds\"] = df[\"rt\"] / 1000\n",
    "sns.scatterplot(x=\"distances\", y=\"rt_seconds\", hue=\"userID\", data=df)\n",
    "plt.ylabel(\"Response Time (s)\")\n",
    "plt.xlabel(\"Distractor Distance\")\n",
    "plt.legend(\"\")\n",
    "plt.title(\"Distractor distance by response time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dists = []\n",
    "response_times = []\n",
    "sample_sizes = []\n",
    "df[\"rt_s\"] = df[\"rt\"] / 1000\n",
    "\n",
    "for (dist), row in df.groupby([\"dist_binned\"]):\n",
    "    dists.append(dist)\n",
    "    response_times.append(row[\"rt_s\"].mean())\n",
    "    sample_sizes.append(len(row))\n",
    "    \n",
    "    \n",
    "tempdf = pd.DataFrame({\"dists\": dists, \"response_times\": response_times,\n",
    "                       \"sample_sizes\": sample_sizes})\n",
    "\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "g = sns.regplot(x=dists, y=response_times, scatter=True, \n",
    "             x_jitter=0.05, line_kws={\"linewidth\": 4})\n",
    "\n",
    "plt.xticks(range(20), [f\"{b:.02f}\" for b in bins], rotation=45)\n",
    "\n",
    "plt.suptitle(f\"Response Time by Distractor Distance\", y=.98)\n",
    "plt.xlabel(\"Binned Distractor Distace\")\n",
    "plt.ylabel(f\"Average Response Times (s)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = []\n",
    "response_times = []\n",
    "sample_sizes = []\n",
    "matching = []\n",
    "\n",
    "for (dist, shape_matching), row in df.groupby([\"dist_binned\", \"shape_matching\"]):\n",
    "    dists.append(dist)\n",
    "    response_times.append(row[\"rt_s\"].mean())\n",
    "    sample_sizes.append(len(row))\n",
    "    matching.append(shape_matching)\n",
    "    \n",
    "    \n",
    "tempdf = pd.DataFrame({\"dists\": dists, \"response_times\": response_times, \"Same-class distractor\": matching, \n",
    "                       \"sample_sizes\": sample_sizes})\n",
    "\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "g = sns.lmplot(x=\"dists\", y=\"response_times\", scatter=True, col=\"Same-class distractor\", hue=\"Same-class distractor\", data=tempdf, \n",
    "           sharex=True, sharey=True, aspect=1.5, x_jitter=0.05, line_kws={\"linewidth\": 4})\n",
    "\n",
    "g.set_axis_labels(\"Binned Distractor Distance\", \"Response Times (s)\").fig.subplots_adjust(wspace=.1)\n",
    "for ax in g.axes.flat:\n",
    "    ax.set_xticks(range(20), [f\"{b:.02f}\" for b in bins], rotation=45)\n",
    "\n",
    "plt.suptitle(f\"Response Times by Distractor Distance\", y=1.05)\n",
    "plt.xlabel(\"Binned Distractor Distace\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
